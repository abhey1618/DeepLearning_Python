{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from keras.datasets import imdb\nimport keras\nimport numpy as np\nkeras.__version__","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"'2.2.4'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(imdb)","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"module"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**DIR Function**\n\nUsing dir() on module object \"imdb\" returns a list of the attributes and methods of any object (say functions , modules, strings, lists, dictionaries etc.)\n\n* For Modules/Library objects, it tries to return a list of names of all the attributes, contained in that module.\n* If no parameters are passed it returns a list of names in the current local scope.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dir(imdb))","execution_count":3,"outputs":[{"output_type":"stream","text":"['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_remove_long_seq', 'absolute_import', 'division', 'get_file', 'get_word_index', 'json', 'load_data', 'np', 'print_function', 'warnings']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**load_data to get training and test data**\n\nThe old version of numpy had **allow_pickle=True** as the default value for **np.load** command which was assumed in keras while importing data. So, either we need to change the np.load command in imdb.py or we can change the default value just for importing the data and after that restore the old."},{"metadata":{"trusted":true},"cell_type":"code","source":"# save np.load\nnp_load_old = np.load\n\n# modify the default parameters of np.load\nnp.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will load data for 10,000 words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# call load_data with allow_pickle implicitly set to true\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n\n# restore np.load for future normal usage\nnp.load = np_load_old","execution_count":5,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n17465344/17464789 [==============================] - 0s 0us/step\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Let us see how our data looks like.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data[0])","execution_count":6,"outputs":[{"output_type":"stream","text":"[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The above are the indices of the words that are being used. The actual words can be found out using get_word_index() attribute of our **module imdb** "},{"metadata":{"trusted":true},"cell_type":"code","source":"word_to_ind = imdb.get_word_index()","execution_count":7,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n1646592/1641221 [==============================] - 0s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_to_word = dict([(value, key) for key,value in word_to_ind.items()])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The {}th and {}th word in the vocabulary are \\{}/ and \\{}/ respectively.\".format(16,22,ind_to_word[16],ind_to_word[22]))\nprint(\"The words \\happy/ and \\sad/ in the vocabulary have {}th and {}th index respectively\".format(word_to_ind['happy'],word_to_ind['sad']))","execution_count":9,"outputs":[{"output_type":"stream","text":"The 16th and 22th word in the vocabulary are \\with/ and \\you/ respectively.\nThe words \\happy/ and \\sad/ in the vocabulary have 651th and 616th index respectively\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We have loaded the data and understood its structure. Now, we need to get our data ready for modelling. First thing to notice is that we need to have each sample of the same shape.\n\n* We can use embeddings of each word and equalize the length of each sentence by using padding.\n* We can one-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, that if there is at least one occurrence of a word in a sentence that word will have 1 against its index and otherwise 0 if there is no occurrence.\n\nFor now, we will be using the latter approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorize_sequences(sequences, dimension=10000):\n\tresults = np.zeros((len(sequences), dimension))\n\tfor i, sequence in enumerate(sequences):\n\t\tresults[i, sequence] = 1.\n\treturn results","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will first try a simple neural network in Keras. Usually, RNN or recurrent neural network works well for language data. There are two ways to implement a model in Keras.\n\n1. Using keras.models.Sequential\n2. Using keras.models.Model\n\nWe will use both of them in the above order"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense\nfrom keras.models import Sequential","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(108, activation = 'relu', input_shape = [10000,]))\nmodel.add(Dense(10, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":14,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 108)               1080108   \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1090      \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 1,081,209\nTrainable params: 1,081,209\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class my_callback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs = {}):\n        if(logs.get('acc') > 0.99):\n            print(\"Stopping to prevent overfitting\")\n            self.model.stop_training = True","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callback = my_callback()\nmodel.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(x_train, y_train, epochs = 20, callbacks = [callback])","execution_count":16,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n25000/25000 [==============================] - 13s 535us/step - loss: 0.3175 - acc: 0.8692\nEpoch 2/20\n25000/25000 [==============================] - 13s 507us/step - loss: 0.1714 - acc: 0.9312\nEpoch 3/20\n25000/25000 [==============================] - 13s 508us/step - loss: 0.1005 - acc: 0.9588\nEpoch 4/20\n25000/25000 [==============================] - 13s 513us/step - loss: 0.0433 - acc: 0.9834\nEpoch 5/20\n25000/25000 [==============================] - 13s 503us/step - loss: 0.0173 - acc: 0.9950\nStopping to prevent overfitting\n","name":"stdout"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"<keras.callbacks.History at 0x7f8e2290b240>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have got 86% accuracy.** But our insample accuracy is more than 99%. We are clearly suffering from overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input\nfrom keras.models import Model","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Input(shape = (10000,))\nY = Dense(108, activation = 'relu')(X)\nY = Dense(10, activation = 'relu')(Y)\nY = Dense(1, activation = 'sigmoid')(Y)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(inputs = [X], outputs = [Y])\nmodel.summary()","execution_count":19,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 10000)             0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 108)               1080108   \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                1090      \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 1,081,209\nTrainable params: 1,081,209\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(x_train, y_train, epochs = 15, callbacks = [callback])","execution_count":21,"outputs":[{"output_type":"stream","text":"Epoch 1/15\n25000/25000 [==============================] - 12s 499us/step - loss: 0.1924 - acc: 0.9232\nEpoch 2/15\n25000/25000 [==============================] - 11s 451us/step - loss: 0.0897 - acc: 0.9658\nEpoch 3/15\n25000/25000 [==============================] - 11s 457us/step - loss: 0.0302 - acc: 0.9902\nStopping to prevent overfitting\n","name":"stdout"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"<keras.callbacks.History at 0x7f8e2a44d080>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(x_test, y_test)","execution_count":22,"outputs":[{"output_type":"stream","text":"25000/25000 [==============================] - 3s 110us/step\n","name":"stdout"},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"[0.5419849581956864, 0.86428]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"This gave 86.5% accuracy. The results are a bit different because of different random initialization of weights."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}